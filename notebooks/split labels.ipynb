{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f19804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging\n",
    "# POS\n",
    "\n",
    "def get_predictor(model):\n",
    "    if model == 'pos':\n",
    "        url = \"https://storage.googleapis.com/allennlp-public-models/elmo-constituency-parser-2020.02.10.tar.gz\"\n",
    "    elif model == 'ner':\n",
    "        url = \"https://storage.googleapis.com/allennlp-public-models/ner-elmo.2021-02-12.tar.gz\"\n",
    "    \n",
    "    return Predictor.from_path(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d36c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dialogue_and_summaries(dialogsum_df, summaries, i):\n",
    "    for dlg in dialogsum_df.loc[i]['dialogue'].split('\\n'):\n",
    "        print(dlg)\n",
    "\n",
    "    print('\\n')\n",
    "    for _, (summ, idx) in enumerate(summaries):\n",
    "        if idx == i:\n",
    "            print(summ)\n",
    "            break\n",
    "\n",
    "def prepare_dataframe():\n",
    "    data_path = os.path.join('..', 'data')\n",
    "    dialogsum_path = os.path.join(data_path, 'dialogsum', 'DialogSum_Data')\n",
    "    test_path = os.path.join(dialogsum_path, 'dialogsum.test.jsonl')\n",
    "    dialogsum_test_df = pd.read_json(test_path, lines=True)\n",
    "    dialogsum_test_df = dialogsum_test_df.rename(columns={\"summary1\": \"summary\"})\n",
    "    dialogsum_test_df['split'] = 'test'\n",
    "    \n",
    "    \n",
    "    dev_path = os.path.join(dialogsum_path, 'dialogsum.dev.jsonl')\n",
    "    dialogsum_dev_df = pd.read_json(dev_path, lines=True)\n",
    "    dialogsum_dev_df['split'] = 'val'\n",
    "\n",
    "    train_path = os.path.join(dialogsum_path, 'dialogsum.train.jsonl')\n",
    "    dialogsum_train_df = pd.read_json(train_path, lines=True)\n",
    "    dialogsum_train_df['split'] = 'train'\n",
    "    \n",
    "    dialogsum_df = pd.concat([dialogsum_train_df, dialogsum_dev_df, dialogsum_test_df])\n",
    "    dialogsum_df.reset_index(inplace=True)\n",
    "    \n",
    "    return dialogsum_df\n",
    "\n",
    "def get_summs_w_2_persons(dialogsum_df):\n",
    "    summaries = []\n",
    "    two_person_dlg_f = dialogsum_df[dialogsum_df['dialogue'].str.contains('#Person3#') == False]\n",
    "    indices = dialogsum_df.index[dialogsum_df['dialogue'].str.contains('#Person3#') == False].tolist()\n",
    "    summaries = [(dialogsum_df.loc[idx]['summary'], idx) for idx in indices]\n",
    "    \n",
    "    return summaries, two_person_dlg_f\n",
    "\n",
    "def split_sentence(pred):\n",
    "    sents = []\n",
    "    sent = []\n",
    "    for word, token in zip(pred['tokens'], pred['pos_tags']):\n",
    "        if token == '.':\n",
    "            sent[-1] += word\n",
    "            sents.append(sent)\n",
    "            sent = []\n",
    "        else:\n",
    "            sent.append(word)\n",
    "            \n",
    "    return [\" \".join(s) for s in sents]\n",
    "\n",
    "def get_sentences(pred):\n",
    "    root = pred['hierplane_tree']['root']\n",
    "    sents = []\n",
    "    for child in pred['hierplane_tree']['root']['children']:\n",
    "        if child['nodeType'] == 'S':\n",
    "            sents.append(child)    \n",
    "    \n",
    "    \n",
    "    return sents if sents else [root]\n",
    "\n",
    "def split_summaries(summaries, predictor):\n",
    "    summaries_split = {}\n",
    "    for summ, i in tqdm(summaries):\n",
    "        out = predictor.predict(summ)\n",
    "        summaries_split[i] = split_sentence(out)\n",
    "\n",
    "    return summaries_split\n",
    "\n",
    "def get_split_summaries(summ_split_path, summaries, predictor, force_rerun=False):\n",
    "    if os.path.exists(summ_split_path) and not force_rerun:\n",
    "        with open(summ_split_path, 'r') as json_file:\n",
    "            splits = json.load(json_file)\n",
    "            summaries_split = {int(k): splits[k] for k in splits}\n",
    "    else:\n",
    "        summaries_split = split_summaries(summaries, predictor)\n",
    "        with open(summ_split_path, 'w') as json_file:\n",
    "            json.dump(summaries_split, json_file)\n",
    "    \n",
    "    return summaries_split\n",
    "\n",
    "def cleanup_summs(summaries_split):\n",
    "    for k in tqdm(summaries_split):\n",
    "        for i in range(len(summaries_split[k])):\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace('#Person1#', 'XYZ1')\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace('# Person1#', 'XYZ1')\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace('# Person1 #', 'XYZ1')\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace('#Person1 #', 'XYZ1')\n",
    "\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace('#Person2#', 'XYZ2')\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace('# Person2#', 'XYZ2')\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace('# Person2 #', 'XYZ2')\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace('#Person2 #', 'XYZ2')\n",
    "\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace(\" 'll\", \"'ll\")\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace(\" 's\", \"'s\")\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace(\" n't\", \"n't\")\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace(\" - \", \"-\")\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace(\" ,\", \",\")\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace(\" 've'\", \"'ve'\")\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace(\"..\", \".\")\n",
    "            summaries_split[k][i] = summaries_split[k][i].replace('. ', ' ')\n",
    "            \n",
    "def tag_summs(summaries_split, predictor):\n",
    "    tagged_summs = {}\n",
    "    curr = None\n",
    "\n",
    "    for k in tqdm(summaries_split):\n",
    "        curr = k\n",
    "        split = summaries_split[k]\n",
    "        tagged_summs[k] = []\n",
    "        for summ in split:\n",
    "            ptree = predict_and_get_tree(summ, predictor)\n",
    "            subs = get_subsentences(ptree)\n",
    "            #subs_2 = []\n",
    "            #for sub in subs:\n",
    "            #    subs_2 += get_subsentences(sub) \n",
    "\n",
    "            clauses = get_clauses(subs)\n",
    "            tagged_summs[k].extend(clauses)\n",
    "    \n",
    "    return tagged_summs\n",
    "\n",
    "def correct_tags(tagged_summs, summaries_split):\n",
    "    count = 0\n",
    "    for k in tagged_summs:\n",
    "        if len(summaries_split[k]) > len(tagged_summs[k]):\n",
    "            count += 1\n",
    "            tagged_summs[k] = summaries_split[k][::]\n",
    "        \n",
    "def cleanup_tags(tagged_summs):\n",
    "    for k in tqdm(tagged_summs):\n",
    "        i = 0\n",
    "        while i < len(tagged_summs[k]):\n",
    "            if tagged_summs[k][i] == '.':\n",
    "                del tagged_summs[k][i]\n",
    "            else:\n",
    "                if isinstance(tagged_summs[k][i], list):\n",
    "                    tagged_summs[k][i] = ' '.join(tagged_summs[k][i])\n",
    "                tagged_summs[k][i] = tagged_summs[k][i].replace(\" 'll\", \"'ll\")\n",
    "                tagged_summs[k][i] = tagged_summs[k][i].replace(\" 's\", \"'s\")\n",
    "                tagged_summs[k][i] = tagged_summs[k][i].replace(\" ,\", \",\")\n",
    "                tagged_summs[k][i] = tagged_summs[k][i].replace(\" '\", \"'\")\n",
    "                tagged_summs[k][i] = tagged_summs[k][i].replace(\" - \", \"-\")\n",
    "                tagged_summs[k][i] = tagged_summs[k][i].replace(\". \", \" \")\n",
    "                if len(tagged_summs[k][i]) > 0:\n",
    "                    if tagged_summs[k][i][-1] != '.':\n",
    "                        tagged_summs[k][i] = tagged_summs[k][i] + '.'\n",
    "                tagged_summs[k][i] = tagged_summs[k][i].replace(\"Person1\", \"XYZ1\")\n",
    "                tagged_summs[k][i] = tagged_summs[k][i].replace(\"Person2\", \"XYZ2\")\n",
    "                i += 1\n",
    "                \n",
    "    # hard coded corrections\n",
    "    tagged_summs[259][0] = 'Alice wants to apply for a scholarship offered by the American Minority Students Scholarship Association since she is eligible for it that she is Asian American, a student in junior year and has GPA 3.92.'\n",
    "    tagged_summs[259].pop(1)\n",
    "    tagged_summs[298][1] = 'They both play bridge'\n",
    "    tagged_summs[2016] = [\"Edward Smith wants to book a flight to New York on July 21st but it isn't available.\", \"he takes another flight on July 22nd.\"]\n",
    "    tagged_summs[744] = [\"XYZ1 sends a necklace to Mom on Mother's Day\"] + tagged_summs[744]\n",
    "    tagged_summs[4446][1] = 'XYZ1 thinks working overtime is not always pleasant.'\n",
    "    tagged_summs[4250][1] = 'Jason comforts her.'\n",
    "    tagged_summs[3363][2] = \"there'll be more collections of his works.\"\n",
    "    tagged_summs[6934] = ['XYZ1 is going to buy bicycle A5, FOB Qingdao from Mr Smith.', 'they agree on 3.5%.']\n",
    "    tagged_summs[8760][1] = \"They 've got meat, utensils and paper plates, and are going to buy some buns and ketchup.\"\n",
    "    tagged_summs[10203][2] = \"they've made a room reservation.\"\n",
    "    tagged_summs[11213][1] = \"They've been dating for three years.\"\n",
    "    tagged_summs[11832][0] = \"Say forgets to take Melber's book and suggest they pick it up after the show.\"\n",
    "    \n",
    "            \n",
    "def get_tagged_summs(tagged_summs_path, summaries_split, predictor, force_rerun=False):\n",
    "    if os.path.exists(tagged_summs_path) and not force_rerun:\n",
    "        with open(tagged_summs_path, 'r') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            tagged_summs = {int(k): json_data[k] for k in json_data}\n",
    "    else:\n",
    "        tagged_summs = tag_summs(summaries_split, predictor)\n",
    "        with open(tagged_summs_path, 'w') as json_file:\n",
    "            json.dump(tagged_summs, json_file)\n",
    "\n",
    "    \n",
    "    correct_tags(tagged_summs, summaries_split)\n",
    "    cleanup_tags(tagged_summs)\n",
    "    remove_dots_and_empty(tagged_summs)\n",
    "    \n",
    "    return tagged_summs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b86b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from nltk import Tree\n",
    "from nltk.tree.parented import ParentedTree\n",
    "\n",
    "preposition_dependent = set([\n",
    "    'if', 'though', 'before', 'although', 'beside', 'besides', 'despite', 'during',\n",
    "    'unless', 'until', 'via', 'vs', 'upon', 'unlike', 'like', 'with', 'within', 'without', 'because'\n",
    "])\n",
    "\n",
    "noun_tags = set(['NP', 'NN', 'NNP'])\n",
    "verb_tags = set(['VP', 'VBP'])\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "def get_clauses(subsentences):\n",
    "    clauses = []\n",
    "    for subsent in subsentences:\n",
    "        clause = []\n",
    "        i = 0\n",
    "        while i < len(subsent):\n",
    "            child = subsent[i]\n",
    "            if child[0] == 'and':\n",
    "                clause.extend(child.leaves())\n",
    "                i += 1\n",
    "                child = subsent[i]\n",
    "                clause.extend(child.leaves())\n",
    "            elif child.label() == 'SBAR':\n",
    "                if child[0].label() == 'IN' and child[0][0].lower() not in preposition_dependent:\n",
    "                    clauses.append(clause)\n",
    "                    clause = []\n",
    "                    clauses.append(child.leaves()[1:])\n",
    "                else:\n",
    "                    clause.extend(child.leaves())\n",
    "\n",
    "            elif child.right_sibling():\n",
    "                if child.label() == 'PP' and child.right_sibling().label() in noun_tags:\n",
    "                    clause.extend(child.leaves())\n",
    "                    \n",
    "                if child.label() in noun_tags:\n",
    "                    clause.extend(child.leaves())\n",
    "                    if child.right_sibling().label() != 'SBAR':\n",
    "                        clause.extend(child.right_sibling().leaves())\n",
    "                    if child.right_sibling().right_sibling():                        \n",
    "                        if child.right_sibling().label() == 'ADVP' :\n",
    "                            clause.extend(child.right_sibling().right_sibling().leaves())\n",
    "                        if child.right_sibling().label() == 'JJ' and child.right_sibling().right_sibling().label() == 'PP':\n",
    "                            clause.extend(child.right_sibling().right_sibling().leaves())\n",
    "                        elif child.right_sibling().right_sibling().label() == 'VP':\n",
    "                            clause.extend(child.right_sibling().right_sibling().leaves())\n",
    "\n",
    "            i += 1\n",
    "        if clause:\n",
    "            clauses.append(clause)\n",
    "\n",
    "    return clauses\n",
    "\n",
    "def get_subsentences(tree):\n",
    "    subs = []\n",
    "    tmp = []\n",
    "    for i, child in enumerate(tree):\n",
    "        if child.label() != 'CC':\n",
    "            tmp.append(child)\n",
    "        if child.label() == 'S':\n",
    "            if tmp:\n",
    "                subs.append(tmp)\n",
    "                tmp = []\n",
    "            subs.append(child)\n",
    "\n",
    "    return subs if subs else [tree]\n",
    "\n",
    "def contains_title(subject):\n",
    "    lower_string = [string.lower() for string in subject]\n",
    "    titles = ['mr', 'mrs', 'ms', 'mister', 'miss', 'misses', 'dr', 'doctor']\n",
    "    for title in titles:\n",
    "        if title in lower_string:\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "    \n",
    "def get_subject(tree):\n",
    "    output = []\n",
    "    for child in tree:\n",
    "        if child.label() in noun_tags:\n",
    "            output = child.leaves()\n",
    "            break\n",
    "        elif child.label() == 'S' or child.label() == 'SBAR':\n",
    "            output = get_subject(child)\n",
    "            if output:\n",
    "                break\n",
    "        elif child.label() == 'VP':\n",
    "            if child.left_sibling():\n",
    "                output = child.left_sibling().leaves()\n",
    "            else:\n",
    "                output = get_subject(child)\n",
    "            break\n",
    "    \n",
    "    if len(output) >= 2:\n",
    "        if 'and' in output:\n",
    "            output = output\n",
    "        elif contains_title(output):\n",
    "\n",
    "            output = [output[1]]\n",
    "        else:\n",
    "            output = [output[0]]\n",
    "        \n",
    "        return output\n",
    "     \n",
    "    return output\n",
    "\n",
    "def predict_and_get_tree(summ, predictor=None):\n",
    "    if predictor is None:\n",
    "        predictor = get_predictor('pos')\n",
    "    pred = predictor.predict(summ)\n",
    "    t = Tree.fromstring(pred['trees'])\n",
    "    ptree = ParentedTree.convert(t)\n",
    "\n",
    "    return ptree\n",
    "\n",
    "def has_top_level_NP(tree):\n",
    "    noun_tags = set(['NP', 'NN', 'NNP'])\n",
    "    for child in tree:\n",
    "        if child.label() == 'S' or child.label() == 'SBAR':\n",
    "            return has_top_level_NP(child)\n",
    "        if child.label() == 'VP' or child.label() in noun_tags:\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def get_names(pred):\n",
    "    names = []\n",
    "    for word, tag in zip(pred['words'], pred['tags']):\n",
    "        if 'PER' in tag:\n",
    "            names.append(word.lower())\n",
    "    \n",
    "    return names\n",
    "\n",
    "def get_all_names(tagged_summ):\n",
    "    names = []\n",
    "    for k in tqdm(tagged_summs):\n",
    "        for sent in tagged_summs[k]:\n",
    "            pred = predictor.predict(sent)\n",
    "            names += get_names(pred)\n",
    "    \n",
    "    names_set = set(names)\n",
    "    names_set.add('xyz1')\n",
    "    names_set.add('xyz2')\n",
    "    return names_set\n",
    "\n",
    "def get_summs_w_they(tagged_summs):\n",
    "    with_they = []\n",
    "    for k in tagged_summs:\n",
    "        for i in range(len(tagged_summs[k])):\n",
    "            if 'they' in tagged_summs[k][i]:\n",
    "                with_they.append((k,i))\n",
    "    \n",
    "    return with_they\n",
    "\n",
    "def get_none_and_theyNP(with_they, tagged_summs, predictor):\n",
    "    they_NP = []\n",
    "    is_none = []\n",
    "    for k, i in tqdm(with_they):\n",
    "        pair = (k, i)\n",
    "        summ = tagged_summs[k][i]\n",
    "        pred = predictor.predict(summ)\n",
    "        t = Tree.fromstring(pred['trees'])\n",
    "        ptree = ParentedTree.convert(t)\n",
    "        subject = get_subject(ptree)\n",
    "        if subject:\n",
    "            if 'they' in subject.lower():\n",
    "                they_NP.append(pair)\n",
    "        else:\n",
    "            is_none.append(pair)\n",
    "            \n",
    "    return they_NP, is_none\n",
    "\n",
    "def distinct_prepositions(summaries, predictor):\n",
    "    preps = set()\n",
    "    for k in tqdm(summaries):\n",
    "        split = summaries[k]\n",
    "        for sent in split:\n",
    "            out = predictor.predict(sentence=sent)\n",
    "            t = Tree.fromstring(out['trees'])\n",
    "            ptree = ParentedTree.convert(t)\n",
    "            for subtree in ptree.subtrees(filter=lambda x: x.label() == 'IN'):\n",
    "                preps.add(subtree[0])\n",
    "    return preps\n",
    "\n",
    "def remove_dots_and_empty(tagged_summs):\n",
    "    for k in tagged_summs:\n",
    "        i = 0\n",
    "        while i < len(tagged_summs[k]):\n",
    "            if tagged_summs[k][i] == '.' or tagged_summs[k][i] == '':\n",
    "                del tagged_summs[k][i]\n",
    "            else:\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03c6e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogsum_df = prepare_dataframe()\n",
    "summaries, dialogsum_df = get_summs_w_2_persons(dialogsum_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a04379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tnguyen/dialogue-text-summarization-dokument/.venv/lib/python3.6/site-packages/allennlp/tango/__init__.py:18: UserWarning: AllenNLP Tango is an experimental API and parts of it might change or disappear every time we release a new version.\n",
      "  \"AllenNLP Tango is an experimental API and parts of it might change or disappear \"\n",
      "2022-06-26 22:32:58,104 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
      "2022-06-26 22:32:58,363 - INFO - allennlp.common.file_utils - cache of https://storage.googleapis.com/allennlp-public-models/elmo-constituency-parser-2020.02.10.tar.gz is up-to-date\n",
      "2022-06-26 22:32:58,364 - INFO - allennlp.models.archival - loading archive file https://storage.googleapis.com/allennlp-public-models/elmo-constituency-parser-2020.02.10.tar.gz from cache at /home/tnguyen/.allennlp/cache/653d0c5a1fb85ac98e84e332fa2a2c0596d9c86a2f38189886d65a422dabe1e9.8cfb67d64c5824347f7328a0f84e46d2e74f9d9bb1aba6441b313d5aaccdea4d\n",
      "2022-06-26 22:32:58,366 - INFO - allennlp.models.archival - extracting archive file /home/tnguyen/.allennlp/cache/653d0c5a1fb85ac98e84e332fa2a2c0596d9c86a2f38189886d65a422dabe1e9.8cfb67d64c5824347f7328a0f84e46d2e74f9d9bb1aba6441b313d5aaccdea4d to temp dir /tmp/tmpizb9ta_v\n",
      "2022-06-26 22:33:03,561 - INFO - allennlp.common.params - dataset_reader.type = ptb_trees\n",
      "2022-06-26 22:33:03,562 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
      "2022-06-26 22:33:03,563 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
      "2022-06-26 22:33:03,564 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
      "2022-06-26 22:33:03,565 - INFO - allennlp.common.params - dataset_reader.token_indexers.elmo.type = elmo_characters\n",
      "2022-06-26 22:33:03,566 - INFO - allennlp.common.params - dataset_reader.token_indexers.elmo.namespace = elmo_characters\n",
      "2022-06-26 22:33:03,567 - INFO - allennlp.common.params - dataset_reader.token_indexers.elmo.tokens_to_add = None\n",
      "2022-06-26 22:33:03,568 - INFO - allennlp.common.params - dataset_reader.token_indexers.elmo.token_min_padding_length = 0\n",
      "2022-06-26 22:33:03,569 - INFO - allennlp.common.params - dataset_reader.use_pos_tags = True\n",
      "2022-06-26 22:33:03,570 - INFO - allennlp.common.params - dataset_reader.convert_parentheses = False\n",
      "2022-06-26 22:33:03,571 - INFO - allennlp.common.params - dataset_reader.label_namespace_prefix = \n",
      "2022-06-26 22:33:03,572 - INFO - allennlp.common.params - dataset_reader.pos_label_namespace = pos\n",
      "2022-06-26 22:33:03,572 - INFO - allennlp.common.params - dataset_reader.type = ptb_trees\n",
      "2022-06-26 22:33:03,573 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
      "2022-06-26 22:33:03,574 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
      "2022-06-26 22:33:03,574 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
      "2022-06-26 22:33:03,575 - INFO - allennlp.common.params - dataset_reader.token_indexers.elmo.type = elmo_characters\n",
      "2022-06-26 22:33:03,575 - INFO - allennlp.common.params - dataset_reader.token_indexers.elmo.namespace = elmo_characters\n",
      "2022-06-26 22:33:03,576 - INFO - allennlp.common.params - dataset_reader.token_indexers.elmo.tokens_to_add = None\n",
      "2022-06-26 22:33:03,576 - INFO - allennlp.common.params - dataset_reader.token_indexers.elmo.token_min_padding_length = 0\n",
      "2022-06-26 22:33:03,577 - INFO - allennlp.common.params - dataset_reader.use_pos_tags = True\n",
      "2022-06-26 22:33:03,577 - INFO - allennlp.common.params - dataset_reader.convert_parentheses = False\n",
      "2022-06-26 22:33:03,578 - INFO - allennlp.common.params - dataset_reader.label_namespace_prefix = \n",
      "2022-06-26 22:33:03,578 - INFO - allennlp.common.params - dataset_reader.pos_label_namespace = pos\n",
      "2022-06-26 22:33:03,579 - INFO - allennlp.common.params - type = from_instances\n",
      "2022-06-26 22:33:03,579 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpizb9ta_v/vocabulary.\n",
      "2022-06-26 22:33:03,583 - INFO - allennlp.common.params - model.type = constituency_parser\n",
      "2022-06-26 22:33:03,584 - INFO - allennlp.common.params - model.regularizer = None\n",
      "2022-06-26 22:33:03,585 - INFO - allennlp.common.params - model.ddp_accelerator = None\n",
      "2022-06-26 22:33:03,585 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
      "2022-06-26 22:33:03,587 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.elmo.type = elmo_token_embedder\n",
      "2022-06-26 22:33:03,588 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.elmo.options_file = https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\n",
      "2022-06-26 22:33:03,588 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.elmo.weight_file = https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\n",
      "2022-06-26 22:33:03,589 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.elmo.do_layer_norm = False\n",
      "2022-06-26 22:33:03,589 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.elmo.dropout = 0.2\n",
      "2022-06-26 22:33:03,590 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.elmo.requires_grad = False\n",
      "2022-06-26 22:33:03,590 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.elmo.projection_dim = None\n",
      "2022-06-26 22:33:03,591 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.elmo.vocab_to_cache = None\n",
      "2022-06-26 22:33:03,591 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.elmo.scalar_mix_parameters = None\n",
      "2022-06-26 22:33:03,592 - INFO - allennlp.modules.elmo - Initializing ELMo\n",
      "2022-06-26 22:33:04,433 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json is up-to-date\n",
      "2022-06-26 22:33:05,190 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5 is up-to-date\n",
      "2022-06-26 22:33:05,985 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5 is up-to-date\n",
      "2022-06-26 22:33:06,760 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5 is up-to-date\n",
      "2022-06-26 22:33:07,525 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5 is up-to-date\n",
      "2022-06-26 22:33:08,342 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5 is up-to-date\n",
      "2022-06-26 22:33:09,129 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5 is up-to-date\n",
      "2022-06-26 22:33:09,906 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5 is up-to-date\n",
      "2022-06-26 22:33:10,714 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5 is up-to-date\n",
      "2022-06-26 22:33:11,606 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5 is up-to-date\n",
      "2022-06-26 22:33:12,431 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5 is up-to-date\n",
      "2022-06-26 22:33:13,239 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5 is up-to-date\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-26 22:33:14,005 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json is up-to-date\n",
      "2022-06-26 22:33:17,236 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5 is up-to-date\n",
      "2022-06-26 22:33:18,386 - INFO - allennlp.common.params - model.span_extractor.type = bidirectional_endpoint\n",
      "2022-06-26 22:33:18,387 - INFO - allennlp.common.params - model.span_extractor.input_dim = 500\n",
      "2022-06-26 22:33:18,388 - INFO - allennlp.common.params - model.span_extractor.forward_combination = y-x\n",
      "2022-06-26 22:33:18,388 - INFO - allennlp.common.params - model.span_extractor.backward_combination = x-y\n",
      "2022-06-26 22:33:18,388 - INFO - allennlp.common.params - model.span_extractor.num_width_embeddings = None\n",
      "2022-06-26 22:33:18,389 - INFO - allennlp.common.params - model.span_extractor.span_width_embedding_dim = None\n",
      "2022-06-26 22:33:18,389 - INFO - allennlp.common.params - model.span_extractor.bucket_widths = False\n",
      "2022-06-26 22:33:18,390 - INFO - allennlp.common.params - model.span_extractor.use_sentinels = True\n",
      "2022-06-26 22:33:18,392 - INFO - allennlp.common.params - model.encoder.type = lstm\n",
      "2022-06-26 22:33:18,392 - INFO - allennlp.common.params - model.encoder.input_size = 1074\n",
      "2022-06-26 22:33:18,393 - INFO - allennlp.common.params - model.encoder.hidden_size = 250\n",
      "2022-06-26 22:33:18,393 - INFO - allennlp.common.params - model.encoder.num_layers = 2\n",
      "2022-06-26 22:33:18,393 - INFO - allennlp.common.params - model.encoder.bias = True\n",
      "2022-06-26 22:33:18,394 - INFO - allennlp.common.params - model.encoder.dropout = 0.2\n",
      "2022-06-26 22:33:18,394 - INFO - allennlp.common.params - model.encoder.bidirectional = True\n",
      "2022-06-26 22:33:18,395 - INFO - allennlp.common.params - model.encoder.stateful = False\n",
      "2022-06-26 22:33:18,420 - INFO - allennlp.common.params - model.feedforward.input_dim = 500\n",
      "2022-06-26 22:33:18,420 - INFO - allennlp.common.params - model.feedforward.num_layers = 1\n",
      "2022-06-26 22:33:18,421 - INFO - allennlp.common.params - model.feedforward.hidden_dims = 250\n",
      "2022-06-26 22:33:18,422 - INFO - allennlp.common.params - model.feedforward.activations = relu\n",
      "2022-06-26 22:33:18,422 - INFO - allennlp.common.params - type = relu\n",
      "2022-06-26 22:33:18,423 - INFO - allennlp.common.params - model.feedforward.dropout = 0.1\n",
      "2022-06-26 22:33:18,425 - INFO - allennlp.common.params - model.pos_tag_embedding.embedding_dim = 50\n",
      "2022-06-26 22:33:18,426 - INFO - allennlp.common.params - model.pos_tag_embedding.num_embeddings = None\n",
      "2022-06-26 22:33:18,426 - INFO - allennlp.common.params - model.pos_tag_embedding.projection_dim = None\n",
      "2022-06-26 22:33:18,426 - INFO - allennlp.common.params - model.pos_tag_embedding.weight = None\n",
      "2022-06-26 22:33:18,427 - INFO - allennlp.common.params - model.pos_tag_embedding.padding_index = None\n",
      "2022-06-26 22:33:18,427 - INFO - allennlp.common.params - model.pos_tag_embedding.trainable = True\n",
      "2022-06-26 22:33:18,428 - INFO - allennlp.common.params - model.pos_tag_embedding.max_norm = None\n",
      "2022-06-26 22:33:18,428 - INFO - allennlp.common.params - model.pos_tag_embedding.norm_type = 2.0\n",
      "2022-06-26 22:33:18,429 - INFO - allennlp.common.params - model.pos_tag_embedding.scale_grad_by_freq = False\n",
      "2022-06-26 22:33:18,429 - INFO - allennlp.common.params - model.pos_tag_embedding.sparse = False\n",
      "2022-06-26 22:33:18,430 - INFO - allennlp.common.params - model.pos_tag_embedding.vocab_namespace = pos\n",
      "2022-06-26 22:33:18,430 - INFO - allennlp.common.params - model.pos_tag_embedding.pretrained_file = None\n",
      "2022-06-26 22:33:18,431 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f43a68ca668>\n",
      "2022-06-26 22:33:18,432 - INFO - allennlp.common.params - model.evalb_directory_path = scripts/EVALB\n",
      "2022-06-26 22:33:18,435 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2022-06-26 22:33:18,435 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2022-06-26 22:33:18,436 - INFO - allennlp.nn.initializers -    encoder._module.bias_hh_l0\n",
      "2022-06-26 22:33:18,436 - INFO - allennlp.nn.initializers -    encoder._module.bias_hh_l0_reverse\n",
      "2022-06-26 22:33:18,437 - INFO - allennlp.nn.initializers -    encoder._module.bias_hh_l1\n",
      "2022-06-26 22:33:18,437 - INFO - allennlp.nn.initializers -    encoder._module.bias_hh_l1_reverse\n",
      "2022-06-26 22:33:18,438 - INFO - allennlp.nn.initializers -    encoder._module.bias_ih_l0\n",
      "2022-06-26 22:33:18,438 - INFO - allennlp.nn.initializers -    encoder._module.bias_ih_l0_reverse\n",
      "2022-06-26 22:33:18,439 - INFO - allennlp.nn.initializers -    encoder._module.bias_ih_l1\n",
      "2022-06-26 22:33:18,440 - INFO - allennlp.nn.initializers -    encoder._module.bias_ih_l1_reverse\n",
      "2022-06-26 22:33:18,440 - INFO - allennlp.nn.initializers -    encoder._module.weight_hh_l0\n",
      "2022-06-26 22:33:18,440 - INFO - allennlp.nn.initializers -    encoder._module.weight_hh_l0_reverse\n",
      "2022-06-26 22:33:18,441 - INFO - allennlp.nn.initializers -    encoder._module.weight_hh_l1\n",
      "2022-06-26 22:33:18,441 - INFO - allennlp.nn.initializers -    encoder._module.weight_hh_l1_reverse\n",
      "2022-06-26 22:33:18,441 - INFO - allennlp.nn.initializers -    encoder._module.weight_ih_l0\n",
      "2022-06-26 22:33:18,442 - INFO - allennlp.nn.initializers -    encoder._module.weight_ih_l0_reverse\n",
      "2022-06-26 22:33:18,442 - INFO - allennlp.nn.initializers -    encoder._module.weight_ih_l1\n",
      "2022-06-26 22:33:18,442 - INFO - allennlp.nn.initializers -    encoder._module.weight_ih_l1_reverse\n",
      "2022-06-26 22:33:18,443 - INFO - allennlp.nn.initializers -    feedforward_layer._module._linear_layers.0.bias\n",
      "2022-06-26 22:33:18,443 - INFO - allennlp.nn.initializers -    feedforward_layer._module._linear_layers.0.weight\n",
      "2022-06-26 22:33:18,443 - INFO - allennlp.nn.initializers -    pos_tag_embedding.weight\n",
      "2022-06-26 22:33:18,444 - INFO - allennlp.nn.initializers -    span_extractor._end_sentinel\n",
      "2022-06-26 22:33:18,444 - INFO - allennlp.nn.initializers -    span_extractor._start_sentinel\n",
      "2022-06-26 22:33:18,444 - INFO - allennlp.nn.initializers -    tag_projection_layer._module.bias\n",
      "2022-06-26 22:33:18,445 - INFO - allennlp.nn.initializers -    tag_projection_layer._module.weight\n",
      "2022-06-26 22:33:18,445 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.input_linearity.weight\n",
      "2022-06-26 22:33:18,445 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.bias\n",
      "2022-06-26 22:33:18,445 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.weight\n",
      "2022-06-26 22:33:18,446 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_projection.weight\n",
      "2022-06-26 22:33:18,446 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.input_linearity.weight\n",
      "2022-06-26 22:33:18,446 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.bias\n",
      "2022-06-26 22:33:18,447 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.weight\n",
      "2022-06-26 22:33:18,447 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_projection.weight\n",
      "2022-06-26 22:33:18,447 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.input_linearity.weight\n",
      "2022-06-26 22:33:18,448 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.bias\n",
      "2022-06-26 22:33:18,448 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-26 22:33:18,448 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_projection.weight\n",
      "2022-06-26 22:33:18,449 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.input_linearity.weight\n",
      "2022-06-26 22:33:18,449 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.bias\n",
      "2022-06-26 22:33:18,449 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.weight\n",
      "2022-06-26 22:33:18,450 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_projection.weight\n",
      "2022-06-26 22:33:18,450 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._char_embedding_weights\n",
      "2022-06-26 22:33:18,450 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.bias\n",
      "2022-06-26 22:33:18,451 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.weight\n",
      "2022-06-26 22:33:18,451 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.bias\n",
      "2022-06-26 22:33:18,451 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.weight\n",
      "2022-06-26 22:33:18,452 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.bias\n",
      "2022-06-26 22:33:18,452 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.weight\n",
      "2022-06-26 22:33:18,452 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.bias\n",
      "2022-06-26 22:33:18,453 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.weight\n",
      "2022-06-26 22:33:18,453 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.bias\n",
      "2022-06-26 22:33:18,453 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.weight\n",
      "2022-06-26 22:33:18,454 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.bias\n",
      "2022-06-26 22:33:18,454 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.weight\n",
      "2022-06-26 22:33:18,454 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.bias\n",
      "2022-06-26 22:33:18,455 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.weight\n",
      "2022-06-26 22:33:18,455 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.bias\n",
      "2022-06-26 22:33:18,455 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.weight\n",
      "2022-06-26 22:33:18,456 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.bias\n",
      "2022-06-26 22:33:18,456 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.weight\n",
      "2022-06-26 22:33:18,456 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.bias\n",
      "2022-06-26 22:33:18,457 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.weight\n",
      "2022-06-26 22:33:18,457 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.gamma\n",
      "2022-06-26 22:33:18,457 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.0\n",
      "2022-06-26 22:33:18,458 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.1\n",
      "2022-06-26 22:33:18,458 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.2\n",
      "2022-06-26 22:33:18,653 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpizb9ta_v\n"
     ]
    }
   ],
   "source": [
    "predictor = get_predictor('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e0698a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42dbfce311514cfcab3ce9440ad2d47c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-26 22:34:49,697 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'pos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff32555dd6c644af9690ec107dc3f3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370beed4157e406081add67414339eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427800d53f8949669f3fab774c7235df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summ_split_path = 'summaries_split.json'\n",
    "summaries_split = get_split_summaries(summ_split_path, summaries, predictor, force_rerun=True)\n",
    "cleanup_summs(summaries_split)\n",
    "\n",
    "tagged_summs_path = 'tagged_summs.json'\n",
    "tagged_summs = get_tagged_summs(tagged_summs_path, summaries_split, predictor, force_rerun=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1896,
   "id": "7c225d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17061f2033e04188ac07b6f862673f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4467442dd9d4de0adea5862214d11fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tagged_summs_path = 'tagged_summs.json'\n",
    "tagged_summs = get_tagged_summs(tagged_summs_path, summaries_split, predictor, force_rerun=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1970,
   "id": "f56bd7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3040753e1c494b5d9354bb0fd59e2ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tagged_summs_path = 'tagged_summs.json'\n",
    "tagged_summs = get_tagged_summs(tagged_summs_path, summaries_split, predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1957,
   "id": "1497761e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XYZ1 made a bargain to buy a new dress.',\n",
       " 'XYZ2 watched TV, read a boring book, and took a shower at home.']"
      ]
     },
     "execution_count": 1957,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_summs[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1922,
   "id": "a08cea47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice wants to apply for a scholarship offered by the American Minority Students Scholarship Association since she is eligible for it that she is Asian American, a student in junior year and has GPA 3.',\n",
       " '92.',\n",
       " 'To get the scholarship, Alice must write an essay on the topic -- The Place of Ethnic Minorities in a Democratic Society.',\n",
       " 'XYZ1 is helping her write a letter of recommendation, read her essay, and give some suggestions.']"
      ]
     },
     "execution_count": 1922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_split[259]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "id": "92b4ca16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75d82cc24df42fb86b34d6eced087cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#summ = summaries_split[258][0]\n",
    "NP_first = True\n",
    "for k in tqdm(range(11213, 13325)):\n",
    "    if k not in tagged_summs:\n",
    "        continue\n",
    "    for i in range(len(tagged_summs[k])):\n",
    "        summ = tagged_summs[k][i]\n",
    "        pred = predictor.predict(summ)\n",
    "        t = Tree.fromstring(pred['trees'])\n",
    "        ptree = ParentedTree.convert(t)\n",
    "        NP_first = has_top_level_NP(ptree)\n",
    "        if (not NP_first):\n",
    "            print(ptree)\n",
    "            break\n",
    "              \n",
    "    if (not NP_first):\n",
    "        print(k, i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "id": "72245d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_summs[165][2] = ' '.join(clauses[0]) + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1260,
   "id": "e6c1ed11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: Hey Kevin, what are you doing here? Don't you usually spend Tuesday nights at home studying?\n",
      "#Person2#: I needed to get out of the house. My parents just went ballistic over something my older sister told them.\n",
      "#Person1#: What did she tell them? Is she dropping out of college?\n",
      "#Person2#: Nothing that serious. She finally told them that she moved out of the dormitory a few months ago and has been living with her boyfriend.\n",
      "#Person1#: And your parents took it badly?\n",
      "#Person2#: That's putting it mildly. My father started shouting at my sister and my mother just glared at her.\n",
      "#Person1#: Ouch, that sounds bad. What did your sister do?\n",
      "#Person2#: She started arguing back to my dad that how much she loves her boyfriend, how they're in love and it's not hurting anybody, and so on. My dad said she's too young to do this, and that she should move out right away.\n",
      "#Person1#: How long has your sister been with her boyfriend?\n",
      "#Person2#: Three years. They've been dating since freshman year. They're even talking about marriage.\n",
      "#Person1#: Really? Then I guess living together would be a good idea.\n",
      "#Person2#: What do you mean?\n",
      "#Person1#: Well, these days too many people are getting divorced. If they live together, then at least they're finding out if they're really compatible or not.\n",
      "#Person2#: I guess so. Better to find out now than after you're married, when it's harder to get out.\n",
      "\n",
      "\n",
      "Kevin tells #Person1# his parents got mad because his sister told them she moved out of the dormitory and has been living with her boyfriend. They've been dating for three years. #Person1# thinks Kevin's sister did the right thing because it's better to find out if they are compatible before marriage.\n"
     ]
    }
   ],
   "source": [
    "i = 11213\n",
    "show_dialogue_and_summaries(dialogsum_df, summaries, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1279,
   "id": "fe69c86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kevin\n"
     ]
    }
   ],
   "source": [
    "summ = tagged_summs[11213][0]\n",
    "ptree = predict_and_get_tree(summ, predictor)\n",
    "subject = get_subject(ptree)\n",
    "print(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1314,
   "id": "88e08fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "two_person_dlg_df = dialogsum_df[dialogsum_df['dialogue'].str.contains('#Person3#') == False]\n",
    "indices = dialogsum_df.index[dialogsum_df['dialogue'].str.contains('#Person3#') == False].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "id": "890641d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogsum_df = prepare_dataframe()\n",
    "summaries, dialogsum_df = get_summs_w_2_persons(dialogsum_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1343,
   "id": "bf2f8941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(11213, 1)}"
      ]
     },
     "execution_count": 1343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_exceptions(tagged_summs):\n",
    "    predictor = get_predictor('pos')\n",
    "    they_NP\n",
    "    for k in tqdm(tagged_summs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1467,
   "id": "9394d527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: They should be a great show. Let's go in.\n",
      "#Person2#: Sure. Say, did you bring my book?\n",
      "#Person1#: Oh, I completely forgot it.\n",
      "#Person2#: You forgot? But you promised. I needed to study for the test. Oh, I knew I never should have lent it to you.\n",
      "#Person1#: Calm down, Melber. After the show, we can drive by my house and pick it up.\n",
      "#Person2#: It's pretty far out of the way. But I guess we'll have to.\n",
      "#Person1#: Don't worry. I'll treat you to an ice cream to make it up to you.\n",
      "#Person2#: OK.\n",
      "\n",
      "\n",
      "Say forgets to take Melber's book and suggest they pick it up after the show.\n"
     ]
    }
   ],
   "source": [
    "show_dialogue_and_summaries(dialogsum_df, summaries, 11832)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2098,
   "id": "82341590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_exceptions(k, i):\n",
    "    p_1 = (True, 'Person1')\n",
    "    p_2 = (True, 'Person2')\n",
    "    exceptions = {\n",
    "        (0, 0): p_1,\n",
    "        (7, 4): p_1,\n",
    "        (950, 0): p_1,\n",
    "        (11213, 1): p_2,\n",
    "        (7233, 1): p_1,\n",
    "        (11832, 0): p_1,\n",
    "    }\n",
    "    is_exception, key = exceptions.get((k, i), (False, None))\n",
    "    \n",
    "    return is_exception, key\n",
    "\n",
    "def assign_labels(tagged_summs, names_set, dialogsum_df, labeled_summs, none_subjects, predictor=None, start_idx=0, end_idx=13460):\n",
    "    #none_subjects = []\n",
    "    #labeled_summs = {}\n",
    "    if predictor is None:\n",
    "        predictor = get_predictor('pos')\n",
    "\n",
    "    for k in tqdm(range(start_idx, end_idx)):\n",
    "        curr[0] = k\n",
    "        if k not in tagged_summs:\n",
    "            continue\n",
    "        labeled_summs[k] = {'Person1': [], 'Person2': []}\n",
    "        prev = None\n",
    "        for i in range(len(tagged_summs[k])):\n",
    "            summ = tagged_summs[k][i]\n",
    "            \n",
    "            is_exception, key = assign_exceptions(k, i)\n",
    "            if is_exception:\n",
    "                labeled_summs[k][key].append(summ)\n",
    "                continue\n",
    "            \n",
    "            tree = predict_and_get_tree(summ, predictor)\n",
    "            \n",
    "            subject = get_subject(tree)\n",
    "            if subject:\n",
    "                subject = ' '.join(subject).lower()\n",
    "                for suffix in [\"'s\", \"'ll\", \"'ve\", \"'\"]:\n",
    "                    subject = subject.replace(suffix, \"\")\n",
    "\n",
    "                if 'and' in subject:\n",
    "                    target_keys = []\n",
    "                    subjects = subject.split('and')\n",
    "                    for subj in subjects:\n",
    "                        target_keys += define_speaker(subj.strip(), names_set, dialogsum_df, k, prev)\n",
    "                else:\n",
    "                    target_keys = define_speaker(subject.lower(), names_set, dialogsum_df, k, prev)\n",
    "                \n",
    "                if not target_keys:\n",
    "                    none_subjects.append((k, i))\n",
    "                    continue\n",
    "                \n",
    "                for key in target_keys:\n",
    "                    labeled_summs[k][key].append(summ)\n",
    "            else:\n",
    "                none_subjects.append((k, i))\n",
    "            prev = key\n",
    "                \n",
    "    return labeled_summs, none_subjects\n",
    "\n",
    "def define_speaker(subject, names_set, dialogsum_df, idx, prev):\n",
    "    if subject == 'xyz1':\n",
    "        keys = ['Person1']\n",
    "    elif subject == 'xyz2':\n",
    "        keys = ['Person2']\n",
    "    elif subject in names_set:\n",
    "        keys = search_speaker(subject, dialogsum_df.loc[idx]['dialogue'])\n",
    "    else:\n",
    "        keys = pronoun_distinction(subject, prev)\n",
    "    \n",
    "    return keys\n",
    "\n",
    "def search_speaker(name, dialogue):\n",
    "    person = None\n",
    "    for utt in dialogue.split('\\n'):\n",
    "        if name in utt.lower():\n",
    "            person = get_speaker(utt)\n",
    "            break\n",
    "        else:\n",
    "            for word in utt.split(' '):\n",
    "                if similar(word.lower(), name) >= 0.75:\n",
    "                    person = get_speaker(utt)\n",
    "                    return [person]\n",
    "                    \n",
    "    \n",
    "    return [person] if person else []\n",
    "\n",
    "\n",
    "def get_speaker(utt):\n",
    "    intro_sents = [\"i am\", \"i'm\", \"name is\", \"name's\", \"this is\", \"that is\", \"that's\"]\n",
    "    introduces = False\n",
    "    for sent in intro_sents:\n",
    "        if sent in utt:\n",
    "            introduces = True\n",
    "            break\n",
    "    speaker = utt.split(' ')[0]\n",
    "\n",
    "    person = 'Person2' if '1' in speaker else 'Person1'\n",
    "    if introduces:\n",
    "        person = 'Person1' if '1' in speaker else 'Person2'\n",
    "\n",
    "    return person\n",
    "\n",
    "def pronoun_distinction(pronoun, prev_label):\n",
    "    singular = set(['he', 'she', 'his', 'her', 'him'])\n",
    "    if pronoun in singular:\n",
    "        return [prev_label] if prev_label else []\n",
    "    \n",
    "    return ['Person1', 'Person2']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2097,
   "id": "ecb6c861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6934: ['XYZ1 is going to buy bicycle A5, FOB Qingdao from Mr Smith.',\n",
       "  'they agree on 3.5%.']}"
      ]
     },
     "execution_count": 2097,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = curr[0]\n",
    "tagged_summs_copy = {i: tagged_summs[i]}\n",
    "tagged_summs_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2096,
   "id": "e54a54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_summs[6934] = ['XYZ1 is going to buy bicycle A5, FOB Qingdao from Mr Smith.', 'they agree on 3.5%.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2065,
   "id": "6f4cd564",
   "metadata": {},
   "outputs": [],
   "source": [
    "none_subjects = []\n",
    "labeled_summs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2079,
   "id": "2193a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "none_subjects_test = []\n",
    "labeled_summs_test = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2101,
   "id": "e0a4f8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fa3c060145484aa5e5b6d795fd83e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6526 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "curr = [0]\n",
    "label_assignments, errors = assign_labels(tagged_summs, names_set, dialogsum_df, labeled_summs, none_subjects, predictor, start_idx=6934)\n",
    "#label_assignments_test, errors_test = assign_labels(tagged_summs_copy, names_set, dialogsum_df, labeled_summs_test, none_subjects_test, predictor, start_idx=i, end_idx=i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2105,
   "id": "63f0e39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 2105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(none_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2107,
   "id": "e3d4708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labels.json', 'w') as json_file:\n",
    "    json.dump(label_assignments, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2111,
   "id": "da4b08b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13283\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k in label_assignments:\n",
    "    if len(label_assignments[k]['Person1']) > 0 or len(label_assignments[k]['Person2']) > 0:\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2db187ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labels.json', 'r') as json_file:\n",
    "    labels = json.load(json_file)\n",
    "    labels = {int(key): labels[key] for key in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e355c143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Person1': ['Frank invites Besty to the party to celebrate his big promotion.'],\n",
       " 'Person2': []}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[max(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "39bb9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in labels:\n",
    "    for person in labels[key]:\n",
    "        for i in range(len(labels[key][person])):\n",
    "            labels[key][person][i] = labels[key][person][i].replace('XYZ1', '#Person1#')\n",
    "            labels[key][person][i] = labels[key][person][i].replace('XYZ2', '#Person2#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cb777fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "empties = []\n",
    "for key in labels:\n",
    "    for person in labels[key]:\n",
    "        if len(labels[key][person]) == 0:\n",
    "            empties.append(key)\n",
    "            labels[key][person].append(dialogsum_df.loc[key]['summary'] + '#TOREMOVE#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "74cb41b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Person1': [\"#Person2# suggests #Person1# use a spam filter to reject Bean's pornographic stuff.#TOREMOVE#\"],\n",
       " 'Person2': [\"#Person2# suggests #Person1# use a spam filter to reject Bean's pornographic stuff.\"]}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c357d6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5615"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1c2d5cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "faulties = set()\n",
    "for k in empties:\n",
    "    if labels[k]['Person1'] == labels[k]['Person2']:\n",
    "        faulties.add(k)\n",
    "\n",
    "faulties = list(faulties)\n",
    "iter_faults = iter(faulties)\n",
    "print(len(faulties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b2a76c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faulties.index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "bd0132ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-ce83bd153a3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_faults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Person1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdialogsum_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "index = next(iter_faults)\n",
    "print(labels[index]['Person1'])\n",
    "print(dialogsum_df.loc[index]['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "4bb3d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in labels:\n",
    "    for person in labels[key]:\n",
    "        if '#TOREMOVE#' in labels[key][person][0]:\n",
    "            labels[key][person][0] = labels[key][person][0].replace('#TOREMOVE#', '')\n",
    "        for i in range(len(labels[key][person])):\n",
    "            if labels[key][person][i][-1] != '.':\n",
    "                labels[key][person][i] += '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2ed36d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Person1': ['Frank invites Besty to the party to celebrate his big promotion.'],\n",
       " 'Person2': [\"Frank invites Besty to the party to celebrate his big promotion. Besty couldn't wait for the party.\"]}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[max(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "83ce802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labels_corrected.json', 'w') as json_file:\n",
    "    json.dump(labels, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3307feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labels_corrected.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    labels = {int(k): data[k] for k in data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "557c7b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "p_1 = le.fit_transform(labels[12]['Person1'])\n",
    "torch.Tensor(p_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "for k in labels:\n",
    "    p_1 = labels[k]['Person1']\n",
    "    p_2 = labels[k]['Person2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "1e1b20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogsum_new_labels = {\n",
    "    'test': {}, 'val': {} , 'train': {}\n",
    "}\n",
    "\n",
    "for key in labels:\n",
    "    split = dialogsum_df.loc[key]['split']\n",
    "    dialogsum_new_labels[split][key] = labels[key]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "7cea0288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Person1': ['Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.'],\n",
       " 'Person2': ['Ms Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.']}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogsum_new_labels['test'][12960]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "caebf797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_labels_to_json(split, data):\n",
    "    with open(f'new_dialogsum_labels_{split}.json', 'w') as json_file:\n",
    "        json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "845c8920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 496\n",
      "val 495\n",
      "train 12333\n"
     ]
    }
   ],
   "source": [
    "for split in dialogsum_new_labels:\n",
    "    data = dialogsum_new_labels[split]\n",
    "    print(split, len(data))\n",
    "    write_labels_to_json(split, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "cc2bdbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 13324\n",
      "test percentage 3.7226058240768536 %\n",
      "val percentage 3.7151005703992794 %\n",
      "train percentage 92.56229360552388 %\n"
     ]
    }
   ],
   "source": [
    "total = len(labels)\n",
    "print('total', total)\n",
    "\n",
    "test_percentage = 496 / total * 100\n",
    "print('test percentage', test_percentage, '%')\n",
    "\n",
    "val_percentage = 495 / total * 100\n",
    "print('val percentage', val_percentage, '%')\n",
    "\n",
    "train_percentage = 12333 / total * 100\n",
    "print('train percentage', train_percentage, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "63d8836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('..', 'data')\n",
    "dialogsum_path = os.path.join(data_path, 'dialogsum', 'DialogSum_Data')\n",
    "test_path = os.path.join(dialogsum_path, 'dialogsum.test.jsonl')\n",
    "dialogsum_test_df = pd.read_json(test_path, lines=True)\n",
    "dialogsum_test_df = dialogsum_test_df.rename(columns={\"summary1\": \"summary\"})\n",
    "dialogsum_test_df['split'] = 'test'\n",
    "\n",
    "\n",
    "dev_path = os.path.join(dialogsum_path, 'dialogsum.dev.jsonl')\n",
    "dialogsum_dev_df = pd.read_json(dev_path, lines=True)\n",
    "dialogsum_dev_df['split'] = 'val'\n",
    "\n",
    "train_path = os.path.join(dialogsum_path, 'dialogsum.train.jsonl')\n",
    "dialogsum_train_df = pd.read_json(train_path, lines=True)\n",
    "dialogsum_train_df['split'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "db0340a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 13460\n",
      "test 500\n",
      "test percentage 3.7147102526002973\n",
      "val 500\n",
      "val percentage 3.7147102526002973\n",
      "train 12460\n",
      "train percentage 92.5705794947994\n"
     ]
    }
   ],
   "source": [
    "def get_percentage(amount, total):\n",
    "    return amount / total * 100\n",
    "\n",
    "len_test = len(dialogsum_test_df)\n",
    "len_val = len(dialogsum_dev_df)\n",
    "len_train = len(dialogsum_train_df)\n",
    "total = len_test + len_val + len_train\n",
    "\n",
    "print('total', total)\n",
    "print('test', len_test)\n",
    "print('test percentage', get_percentage(len_test, total))\n",
    "\n",
    "print('val', len_val)\n",
    "print('val percentage', get_percentage(len_val, total))\n",
    "\n",
    "print('train', len_train)\n",
    "print('train percentage', get_percentage(len_train, total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68d4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
